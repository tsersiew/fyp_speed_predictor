{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM,Dropout,Dense \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set up dataset functions\n",
    "Class has 3 elements\n",
    "- An ```__init__``` function that sets up your class and all the necessary parameters.\n",
    "- An ```__len__``` function that returns the size of your dataset.\n",
    "- An ```__getitem__``` function that given an index within the limits of the size of the dataset returns the associated image and label in tensor form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder names\n",
    "dc_folders = ['Europe', 'Japan', 'USA', 'Upsample']\n",
    "\n",
    "# shuffling data \n",
    "LENDATA = 36 + 6 + 4 + 1 # number of driving cycles = 47\n",
    "np.random.seed(42)\n",
    "p = np.random.permutation(int(LENDATA-1))\n",
    "\n",
    "# Implement the dataset class\n",
    "class DrivingCyclesDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 path_to_dc,\n",
    "                 train=True):\n",
    "        # path_to_dc: where you put the driving cycles dataset\n",
    "        # train: return training set or test set\n",
    "        \n",
    "        # Load all the driving cycles\n",
    "        alldata = []\n",
    "        dcnames = []\n",
    "        if (train == True):\n",
    "            mat = loadmat('./DrivingCycles/WLTPextended.mat')\n",
    "            df = pd.DataFrame(mat['V_z'], columns = ['V_z']) # velocity \n",
    "            df2 = pd.DataFrame(mat['T_z'], columns = ['T_z']) # time\n",
    "            df3 = pd.DataFrame(mat['D_z'], columns = ['D_z']) # acceleration\n",
    "            df = pd.concat([df, df2, df3], axis=1)\n",
    "            alldata.append(df)\n",
    "            dcnames.append('WLTPextended.mat')\n",
    "            for folder in dc_folders:\n",
    "                image_path = os.path.join(path_to_dc, folder)\n",
    "                files = glob.glob(image_path + '/*.mat')\n",
    "                for f in files:\n",
    "                    mat = loadmat(f)\n",
    "                    df = pd.DataFrame(mat['V_z'], columns = ['V_z'])\n",
    "                    df2 = pd.DataFrame(mat['T_z'], columns = ['T_z'])\n",
    "                    df3 = pd.DataFrame(mat['D_z'], columns = ['D_z'])\n",
    "                    df = pd.concat([df, df2, df3], axis=1)\n",
    "                    dcnames.append(os.path.basename(f))\n",
    "                    # each dataframe is a driving cycle \n",
    "                    alldata.append(df)\n",
    "            # Extract the driving cycles with the specified file indexes     \n",
    "            self.data = (np.array(alldata, dtype=object))[p] #numpy array of dataframes \n",
    "            self.names = (np.array(dcnames, dtype=object))[p]\n",
    "        \n",
    "        else:\n",
    "            image_path = os.path.join(path_to_dc, 'test')\n",
    "            files = glob.glob(image_path + '/*.mat')\n",
    "            for f in files:\n",
    "                mat = loadmat(f)\n",
    "                df = pd.DataFrame(mat['V_z'], columns = ['V_z'])\n",
    "                df2 = pd.DataFrame(mat['T_z'], columns = ['T_z'])\n",
    "                df3 = pd.DataFrame(mat['D_z'], columns = ['D_z'])\n",
    "                df = pd.concat([df, df2, df3], axis=1)\n",
    "                dcnames.append(os.path.basename(f))\n",
    "                # each dataframe is a driving cycle \n",
    "                alldata.append(df)\n",
    "\n",
    "            self.data = alldata\n",
    "            self.names = dcnames\n",
    "\n",
    "\n",
    "    def __len__(self, idx):\n",
    "        # Return the number of samples in a driving cycle \n",
    "        return (self.data[idx]).size\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Get an item using its index\n",
    "        # Return the driving cycle and its name \n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, h, f, test):\n",
    "    x = [] #append the last h values\n",
    "    y = [] #append the future f value \n",
    "    for df in dataset:\n",
    "        features_considered = ['V_z', 'D_z']\n",
    "        # features_considered = ['V_z']\n",
    "        features = df[features_considered]\n",
    "        for i in range(h, df.shape[0]-f):\n",
    "            # for each driving cycle dataframe, have sets of (h+f) values \n",
    "            # h values are past values, f values are future value \n",
    "            features['v_ave'] = df['V_z'][i-h:i].mean()\n",
    "            features['v_max'] = df['V_z'][i-h:i].max()\n",
    "            features['v_min'] = df['V_z'][i-h:i].min()\n",
    "            features['a_ave'] = df['D_z'][i-h:i].mean()\n",
    "            features['a_max'] = df['D_z'][i-h:i].max()\n",
    "            features['a_min'] = df['D_z'][i-h:i].min()\n",
    "            x.append(features[i-h:i])\n",
    "            if (test == False):\n",
    "                y.append(df['V_z'][i:i+f])\n",
    "            else:\n",
    "                y.append(df['V_z'][i])\n",
    "    x = np.array(x) \n",
    "    y = np.array(y)  \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Exploring the dataset\n",
    "\n",
    "### Step 2.1: Data visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading datasets\n",
    "dc_path = './DrivingCycles/'\n",
    "dataset_train  = DrivingCyclesDataset(dc_path, train=True)\n",
    "dataset_test = DrivingCyclesDataset(dc_path, train=False)\n",
    "\n",
    "# Plot 1 sample from the test set\n",
    "sample_data = dataset_test.data[0]\n",
    "sample_name = dataset_test.names[0]\n",
    "plt.title(sample_name + \" Driving Cycle\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Velocity (m/s)\")\n",
    "plt.plot(sample_data['T_z'], sample_data['V_z'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2 Load training and test datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the datasets \n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "for df in dataset_train: \n",
    "    df['V_z'] = scaler.fit_transform(df[['V_z']])\n",
    "    df['D_z'] = scaler.fit_transform(df[['D_z']])\n",
    "for df in dataset_test: \n",
    "    df['V_z'] = scaler.fit_transform(df[['V_z']])\n",
    "    df['D_z'] = scaler.fit_transform(df[['D_z']])\n",
    "\n",
    "# parameters h and f\n",
    "h = 15 # length of historical sequence\n",
    "f = 5 # length of forecast sequence \n",
    "\n",
    "# create training set and test set \n",
    "pd.options.mode.chained_assignment = None\n",
    "x_train, y_train = create_dataset(dataset_train, h, f, test=False)\n",
    "x_test, y_test = create_dataset(dataset_test, h, f, test=True)\n",
    "\n",
    "# check \n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "# reshaping input to LSTM model \n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], x_train.shape[2]))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], x_test.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: LSTM\n",
    "In this section we will try to make a LSTM predictor to predict the future velocity. \n",
    "\n",
    "### Step 3.1: Hyperparameter Search \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import keras\n",
    "from keras_tuner import BayesianOptimization\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# defining model space search\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    # tuning number of neurons\n",
    "    model.add(LSTM(hp.Int('first_units',min_value=32,max_value=256,step=32),return_sequences=True, input_shape=(x_train.shape[1],x_train.shape[2])))\n",
    "    model.add(Dropout(hp.Float(f'Dropout_rate',min_value=0,max_value=0.5,step=0.1)))\n",
    "    # tuning number of layers\n",
    "    for i in range(hp.Int('n_layers', 1, 3)):\n",
    "        model.add(LSTM(hp.Int(f'lstm_{i}_units',min_value=32,max_value=256,step=32),return_sequences=True))\n",
    "        model.add(Dropout(hp.Float(f'Dropout_{i}_rate',min_value=0,max_value=0.5,step=0.1)))\n",
    "    model.add(LSTM(hp.Int('last_units',min_value=32,max_value=256,step=32)))\n",
    "    # dropout layer\n",
    "    model.add(Dropout(hp.Float('Dropout_last_rate',min_value=0,max_value=0.5,step=0.1)))\n",
    "    # dense layer \n",
    "    model.add(Dense(y_train.shape[1]))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam',metrics = ['mse'])\n",
    "    return model\n",
    "\n",
    "# create Tuner object\n",
    "tuner= BayesianOptimization(\n",
    "        build_model,\n",
    "        objective='mse',\n",
    "        max_trials=10,\n",
    "        executions_per_trial=1,\n",
    "        directory=os.path.normpath('./keras_tuning'),\n",
    "        project_name='kerastuner_bayesian',\n",
    "        overwrite=True\n",
    "        )\n",
    "\n",
    "# hyperparameter search \n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience = 3, restore_best_weights=True)\n",
    "tuner.search(x_train, y_train,epochs=5, steps_per_epoch = 200,\n",
    "     validation_split=0.2,verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "# best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "print(best_model.summary())\n",
    "\n",
    "# best hyperparameter\n",
    "best_hyperparameters = tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values\n",
    "print('HyperParameters: {}'.format(best_hyperparameters))\n",
    "\n",
    "# prediction \n",
    "predictions=best_model.predict(x_test)\n",
    "\n",
    "# saving model \n",
    "best_model.save('bestLSTM.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### old model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(): \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=32, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2]))) # units depends on create_dataset function\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=256,return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=256,return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=256,return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=f))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2: Load and compile model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('bestLSTM.h5') \n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# check\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2: Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping \n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience = 3, restore_best_weights=True)\n",
    "\n",
    "# model checkpoint \n",
    "model_checkpoint = ModelCheckpoint(filepath=\"mymodel_{epoch}.h6\", save_best_only=True, monitor='val_loss', verbose=1)\n",
    "\n",
    "# Prepare a directory to store all the checkpoints.\n",
    "checkpoint_dir = \"./ckpt\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "def make_or_restore_model():\n",
    "    # Either restore the latest model, or create a fresh one\n",
    "    # if there is no checkpoint available.\n",
    "    checkpoints = [checkpoint_dir + \"/\" + name for name in os.listdir(checkpoint_dir)]\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = max(checkpoints, key=os.path.getctime)\n",
    "        print(\"Restoring from\", latest_checkpoint)\n",
    "        return keras.models.load_model(latest_checkpoint)\n",
    "    print(\"Creating a new model\")\n",
    "    model = load_model('bestLSTM.h5') \n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "model = make_or_restore_model()\n",
    "\n",
    "# start training\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=50, validation_split=0.2, callbacks=[early_stopping, model_checkpoint])\n",
    "model.save('speed_prediction.h5')\n",
    "\n",
    "# load the model \n",
    "model = load_model('speed_prediction.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units=32, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2]))) # units depends on create_dataset function\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=256,return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=256,return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=256,return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=f))\n",
    "\n",
    "# compile the model \n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "# early stopping \n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience = 3, restore_best_weights=True)\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=50, validation_split=0.2, callbacks=[early_stopping])\n",
    "model.save('speed_prediction.h5')\n",
    "\n",
    "# load the model \n",
    "model = load_model('speed_prediction.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.3: Deploy the trained model onto the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "\n",
    "# check\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_scaled = scaler.inverse_transform(y_test.reshape(-1,1))\n",
    "\n",
    "#check\n",
    "print(y_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.4: Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(x_test, y_test, batch_size=50)\n",
    "print(\"Min-max scaled MSE loss: \", results)\n",
    "\n",
    "# other metrics\n",
    "def evaluate_unscaled (predictions, y_test_scaled): \n",
    "    mse_ = keras.losses.MeanSquaredError()\n",
    "\n",
    "    mse = mse_(predictions,y_test_scaled)\n",
    "    print('Transformed MSE loss:', mse.numpy())\n",
    "\n",
    "evaluate_unscaled(predictions, y_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.5 Time lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_shift = 0 \n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "\n",
    "# introduce lag time in predictions\n",
    "for i in range(5):\n",
    "    predictions_shifted = predictions_df.shift(i)\n",
    "    mae_ = keras.losses.MeanAbsoluteError()\n",
    "    time_shift = min(time_shift, mae_(predictions_shifted.to_numpy(),y_test_scaled))\n",
    "print(time_shift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.6: Perturbation of input driving cycle parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturbation \n",
    "def var_importance(model):\n",
    "    for i in range(x_test.shape[2]):  # iterate over the three features\n",
    "        new_x = x_test.copy()\n",
    "        perturbation = np.random.normal(0.0, 0.5, size=new_x.shape[:2])\n",
    "        new_x[:, :, i] = new_x[:, :, i] + perturbation\n",
    "        perturbed_out = model.predict(new_x)\n",
    "        effect = ((predictions - perturbed_out) ** 2).mean() ** 0.5\n",
    "        print(f'Variable {i+1}, perturbation effect: {effect:.4f}')\n",
    "\n",
    "var_importance(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.7: Visualisation of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combination information\n",
    "combi = 'S6'\n",
    "\n",
    "# plotting\n",
    "fig, ax = plt.subplots(figsize=(16,4))\n",
    "ax.plot(y_test_scaled)\n",
    "# plt.plot(predictions, color='red') \n",
    "for i in range(predictions.shape[0]): \n",
    "    plt.plot(range(i,i+f), predictions[i], color='red') #uncomment for all forecast sequences\n",
    "plt.legend(['Actual velocity', 'Predicted velocity'])\n",
    "title = 'Predicted future velocity profile for h=' + str(h) + ' and f=' + str(f) + ' for combination ' + str(combi)\n",
    "plt.title(title)\n",
    "plt.xlabel(\"Velocity (m/s)\")\n",
    "plt.ylabel(\"Time (s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Baseline model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model \n",
    "# assumption that the forecast is equal to the mean of the historical time sequence \n",
    "def create_baseline_results(dataset, h, f):\n",
    "    y = []  \n",
    "    y_true = []\n",
    "    for df in dataset:\n",
    "        for i in range(h, df.shape[0]-f):\n",
    "            # for each driving cycle dataframe, have sets of 51 values \n",
    "            # h values are past values, f values are future value \n",
    "            features = df['V_z']\n",
    "            features['V_z'] = df['V_z'][i-h:i].mean()\n",
    "            y.append(features[i-h:i])\n",
    "            y_true.append(df['V_z'][i:i+f])\n",
    "    y = np.array(y)  \n",
    "    y_true = np.array(y_true) \n",
    "    return y, y_true\n",
    "\n",
    "dc_path = './DrivingCycles/'\n",
    "dataset_test = DrivingCyclesDataset(dc_path, train=False)\n",
    "\n",
    "# parameters h and f\n",
    "h = 10 # length of historical sequence\n",
    "f = 10 # length of forecast sequence \n",
    "\n",
    "# create training set and test set \n",
    "pd.options.mode.chained_assignment = None\n",
    "predictions, y_hat = create_baseline_results(dataset_test, h, f)\n",
    "\n",
    "# evaluating the model\n",
    "print(predictions.shape)\n",
    "print(y_hat.shape)\n",
    "mse_ = tf.keras.losses.MeanSquaredError()\n",
    "mse = mse_(predictions,y_hat)\n",
    "print('mse:', mse)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
