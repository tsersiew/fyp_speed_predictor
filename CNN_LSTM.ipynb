{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import layers\n",
    "from keras import activations\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the data.\n",
    "Class has 3 elements\n",
    "- An ```__init__``` function that sets up your class and all the necessary parameters.\n",
    "- An ```__len__``` function that returns the size of your dataset.\n",
    "- An ```__getitem__``` function that given an index within the limits of the size of the dataset returns the associated image and label in tensor form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder names\n",
    "dc_folders = ['Europe', 'Japan', 'USA', 'Upsample']\n",
    "\n",
    "# Implement the dataset class\n",
    "class DrivingCyclesDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 path_to_dc,\n",
    "                 train=True):\n",
    "        # path_to_dc: where you put the driving cycles dataset\n",
    "        # idxs_train: training set indexes\n",
    "        # idxs_test: test set indexes\n",
    "        # train: return training set or test set\n",
    "        \n",
    "        # Load all the driving cycles\n",
    "        alldata = []\n",
    "        dcnames = []\n",
    "        if (train == True):\n",
    "            mat = loadmat('./DrivingCycles/WLTPextended.mat')\n",
    "            df = pd.DataFrame(mat['V_z'], columns = ['V_z']) # velocity \n",
    "            df2 = pd.DataFrame(mat['T_z'], columns = ['T_z']) # time\n",
    "            df3 = pd.DataFrame(mat['D_z'], columns = ['D_z']) # acceleration\n",
    "            df = pd.concat([df, df2, df3], axis=1)\n",
    "            alldata.append(df)\n",
    "            dcnames.append('WLTPextended.mat')\n",
    "            for folder in dc_folders:\n",
    "                image_path = os.path.join(path_to_dc, folder)\n",
    "                files = glob.glob(image_path + '/*.mat')\n",
    "                for f in files:\n",
    "                    mat = loadmat(f)\n",
    "                    df = pd.DataFrame(mat['V_z'], columns = ['V_z'])\n",
    "                    df2 = pd.DataFrame(mat['T_z'], columns = ['T_z'])\n",
    "                    df3 = pd.DataFrame(mat['D_z'], columns = ['D_z'])\n",
    "                    df = pd.concat([df, df2, df3], axis=1)\n",
    "                    dcnames.append(os.path.basename(f))\n",
    "                    # each dataframe is a driving cycle \n",
    "                    alldata.append(df)\n",
    "            # Extract the driving cycles with the specified file indexes     \n",
    "            self.data = (np.array(alldata, dtype=object))[p] #numpy array of dataframes \n",
    "            self.names = (np.array(dcnames, dtype=object))[p]\n",
    "        \n",
    "        else:\n",
    "            image_path = os.path.join(path_to_dc, 'test')\n",
    "            files = glob.glob(image_path + '/*.mat')\n",
    "            for f in files:\n",
    "                mat = loadmat(f)\n",
    "                df = pd.DataFrame(mat['V_z'], columns = ['V_z'])\n",
    "                df2 = pd.DataFrame(mat['T_z'], columns = ['T_z'])\n",
    "                df3 = pd.DataFrame(mat['D_z'], columns = ['D_z'])\n",
    "                df = pd.concat([df, df2, df3], axis=1)\n",
    "                dcnames.append(os.path.basename(f))\n",
    "                # each dataframe is a driving cycle \n",
    "                alldata.append(df)\n",
    "\n",
    "            self.data = alldata\n",
    "            self.names = dcnames\n",
    "\n",
    "\n",
    "    def __len__(self, idx):\n",
    "        # Return the number of samples in a driving cycle \n",
    "        return (self.data[idx]).size\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Get an item using its index\n",
    "        # Return the driving cycle and its name \n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(lendata, percentage=0.8):\n",
    "    idxs_train = int(percentage * lendata)\n",
    "    idxs_test = idxs_train + 1 \n",
    "    return idxs_train, idxs_test\n",
    "\n",
    "LENDATA = 36 + 6 + 4 + 1 # number of driving cycles = 47\n",
    "np.random.seed(42)\n",
    "idxs_train, idxs_test = split_train_test(LENDATA,0.8)\n",
    "idxs_test = 22 # only 1 test driving cycle for easier visualisation\n",
    "p = np.random.permutation(int(LENDATA-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, h, f, test):\n",
    "    x = [] #append the last 50 values\n",
    "    y = [] #append the future value \n",
    "    for df in dataset:\n",
    "        features_considered = ['V_z', 'D_z']\n",
    "        # features_considered = ['V_z']\n",
    "        features = df[features_considered]\n",
    "        for i in range(h, df.shape[0]-f):\n",
    "            x.append(features[i-h:i])\n",
    "            if (test == False):\n",
    "                y.append(df['V_z'][i:i+f])\n",
    "            else:\n",
    "                y.append(df['V_z'][i])\n",
    "    x = np.array(x) \n",
    "    y = np.array(y)  \n",
    "    # x = np.asarray(x).astype(np.float32)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Explore the data. \n",
    "\n",
    "### Step 2.1: Data visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading datasets\n",
    "dc_path = './DrivingCycles/'\n",
    "dataset_train  = DrivingCyclesDataset(dc_path, train=True)\n",
    "dataset_test = DrivingCyclesDataset(dc_path, train=False)\n",
    "\n",
    "# Plot 1 sample from the test set\n",
    "sample_data = dataset_test.data[0]\n",
    "sample_name = dataset_test.names[0]\n",
    "plt.title(sample_name + \" Driving Cycle\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Velocity (m/s)\")\n",
    "plt.plot(sample_data['T_z'], sample_data['V_z'])\n",
    "plt.show()\n",
    "\n",
    "# scaling the datasets \n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "for df in dataset_train: \n",
    "    df['V_z'] = scaler.fit_transform(df[['V_z']])\n",
    "    df['D_z'] = scaler.fit_transform(df[['D_z']])\n",
    "for df in dataset_test: \n",
    "    df['V_z'] = scaler.fit_transform(df[['V_z']])\n",
    "    df['D_z'] = scaler.fit_transform(df[['D_z']])\n",
    "\n",
    "# parameters h and f\n",
    "h = 15 # length of historical sequence\n",
    "f = 5 # length of forecast sequence \n",
    "\n",
    "# create training set and test set \n",
    "pd.options.mode.chained_assignment = None\n",
    "x_train, y_train = create_dataset(dataset_train, h, f, test=False)\n",
    "x_test, y_test = create_dataset(dataset_test, h, f, test=True)\n",
    "\n",
    "# check \n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "# reshaping input to LSTM model \n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], x_train.shape[2]))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], x_test.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: LSTM\n",
    "In this section we will try to make a LSTM predictor to predict the future velocity. \n",
    "\n",
    "### Step 3.1: Define the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    layers.Conv1D(filters=50,\n",
    "                  kernel_size=5,\n",
    "                  strides=1,\n",
    "                   padding=\"causal\",\n",
    "                   activation=\"relu\",\n",
    "                   input_shape=x_train.shape[-2:]),\n",
    "    layers.MaxPooling1D(pool_size=2, strides=1, padding=\"valid\"),\n",
    "    layers.Conv1D(filters=32, kernel_size=3, strides=1, padding=\"causal\", activation=\"relu\"),\n",
    "    layers.MaxPooling1D(pool_size=2, strides=1, padding=\"valid\"),\n",
    "    layers.LSTM(128, return_sequences=True),\n",
    "    layers.LSTM(128, return_sequences=True),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(units=f)\n",
    "])\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2: Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience = 3, restore_best_weights=True)\n",
    "\n",
    "# model.compile(loss = \"mse\", optimizer = 'adam', metrics = ['mae', 'mse'])\n",
    "model.compile(loss = \"mse\", optimizer = 'adam')\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=50, callbacks=[early_stopping])\n",
    "model.save('speed_prediction_cnn.h5')\n",
    "\n",
    "# load the model \n",
    "model = load_model('speed_prediction_cnn.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.3: Deploy the trained model onto the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "\n",
    "# check\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_scaled = scaler.inverse_transform(y_test.reshape(-1,1))\n",
    "\n",
    "#check\n",
    "print(y_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.5: Evaluate the performance of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(x_test, y_test, batch_size=50)\n",
    "print(\"test loss: \", results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.6: Visualisation of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combination information\n",
    "combi = 'S6'\n",
    "\n",
    "# plotting\n",
    "fig, ax = plt.subplots(figsize=(16,4))\n",
    "ax.plot(y_test_scaled)\n",
    "# plt.plot(predictions, color='red') \n",
    "for i in range(predictions.shape[0]): \n",
    "    plt.plot(range(i,i+f), predictions[i], color='red') \n",
    "plt.legend(['Actual velocity', 'Predicted velocity'])\n",
    "title = 'Predicted future velocity profile for h=' + str(h) + ' and f=' + str(f) + ' for combination ' + str(combi)\n",
    "plt.title(title)\n",
    "plt.xlabel(\"Velocity (m/s)\")\n",
    "plt.ylabel(\"Time (s)\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
